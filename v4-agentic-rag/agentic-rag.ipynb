{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31db6a5c",
   "metadata": {},
   "source": [
    "# Introduction to LangGraph\n",
    "## Building your first graph\n",
    "Let's build a simple graph to see how these pieces fit together. This example creates a basic question-answering system with optional retrieval:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c645185f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "from typing import TypedDict, Literal\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "class SimpleState(TypedDict):\n",
    "    question: str\n",
    "    needs_search: bool\n",
    "    context: str\n",
    "    answer: str\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Verify the API key is loaded\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    raise ValueError(\"OPENAI_API_KEY not found in environment variables\")\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "def classify_question(state: SimpleState) -> SimpleState:\n",
    "    \"\"\"Decide if we need to search for information.\"\"\"\n",
    "    response = llm.invoke(\n",
    "        f\"Does this question need external information? Answer yes/no: {state['question']}\"\n",
    "    )\n",
    "    needs_search = \"yes\" in response.content.lower()\n",
    "    return {\"needs_search\": needs_search}\n",
    "\n",
    "def search_info(state: SimpleState) -> SimpleState:\n",
    "    \"\"\"Simulate searching for information.\"\"\"\n",
    "    # In real code, this would query a vector database\n",
    "    context = f\"Retrieved context for: {state['question']}\"\n",
    "    return {\"context\": context}\n",
    "\n",
    "def generate_answer(state: SimpleState) -> SimpleState:\n",
    "    \"\"\"Generate the final answer.\"\"\"\n",
    "    context = state.get(\"context\", \"No additional context.\")\n",
    "    response = llm.invoke(\n",
    "        f\"Question: {state['question']}\\nContext: {context}\\nAnswer:\"\n",
    "    )\n",
    "    return {\"answer\": response.content}\n",
    "\n",
    "def route_after_classify(state: SimpleState) -> Literal[\"search\", \"generate\"]:\n",
    "    \"\"\"Route based on classification.\"\"\"\n",
    "    if state[\"needs_search\"]:\n",
    "        return \"search\"\n",
    "    return \"generate\"\n",
    "\n",
    "# Build the graph\n",
    "graph = StateGraph(SimpleState)\n",
    "\n",
    "graph.add_node(\"classify\", classify_question)\n",
    "graph.add_node(\"search\", search_info)\n",
    "graph.add_node(\"generate\", generate_answer)\n",
    "\n",
    "graph.set_entry_point(\"classify\")\n",
    "graph.add_conditional_edges(\"classify\", route_after_classify)\n",
    "graph.add_edge(\"search\", \"generate\")\n",
    "graph.add_edge(\"generate\", END)\n",
    "\n",
    "# Compile and run\n",
    "app = graph.compile()\n",
    "result = app.invoke({\"question\": \"What is the capital of France?\"})\n",
    "print(result[\"answer\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4538231",
   "metadata": {},
   "source": [
    "## Visualizing graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e20a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import grandalf  # Install first with `uv pip install grandalf`\n",
    "\n",
    "# ASCII visualization\n",
    "print(app.get_graph().draw_ascii())\n",
    "\n",
    "# Or get a Mermaid diagram\n",
    "print(app.get_graph().draw_mermaid())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ldfuoio08hd",
   "metadata": {},
   "source": [
    "# The Agentic RAG Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hfziuy1myxq",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Annotated, Literal\n",
    "from langgraph.graph import StateGraph, END, add_messages\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import Distance, VectorParams\n",
    "import json\n",
    "import re\n",
    "\n",
    "# Initialize embeddings\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "# Create in-memory Qdrant client\n",
    "qdrant_client = QdrantClient(\":memory:\")\n",
    "\n",
    "# Collection name\n",
    "COLLECTION_NAME = \"study_materials\"\n",
    "\n",
    "# Create collection with proper dimensions (1536 for text-embedding-3-small)\n",
    "qdrant_client.create_collection(\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    vectors_config=VectorParams(size=1536, distance=Distance.COSINE)\n",
    ")\n",
    "\n",
    "# Create LangChain vector store wrapper\n",
    "vector_store = QdrantVectorStore(\n",
    "    client=qdrant_client,\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    embedding=embeddings\n",
    ")\n",
    "\n",
    "print(f\"Created Qdrant collection: {COLLECTION_NAME}\")\n",
    "\n",
    "class AgenticRAGState(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    query: str\n",
    "    query_type: str              # factual, conceptual, procedural\n",
    "    complexity: str              # simple, moderate, complex\n",
    "    needs_retrieval: bool\n",
    "    search_queries: list[str]    # Generated search queries\n",
    "    retrieved_docs: list         # Documents from vector store\n",
    "    retrieval_sufficient: bool   # Did we get enough?\n",
    "    response: str\n",
    "    confidence: float            # 0-1 confidence score\n",
    "    iteration: int               # Track retrieval iterations\n",
    "\n",
    "\n",
    "def parse_json_response(text: str, default: dict = None) -> dict:\n",
    "    \"\"\"Extract and parse JSON from LLM response, handling markdown code blocks.\"\"\"\n",
    "    if not text or not text.strip():\n",
    "        return default or {}\n",
    "    \n",
    "    # Try to find JSON in code blocks first\n",
    "    code_block_match = re.search(r'```(?:json)?\\s*([\\s\\S]*?)```', text)\n",
    "    if code_block_match:\n",
    "        text = code_block_match.group(1).strip()\n",
    "    \n",
    "    # Try to find JSON object or array\n",
    "    json_match = re.search(r'(\\{[\\s\\S]*\\}|\\[[\\s\\S]*\\])', text)\n",
    "    if json_match:\n",
    "        text = json_match.group(1)\n",
    "    \n",
    "    try:\n",
    "        return json.loads(text)\n",
    "    except json.JSONDecodeError:\n",
    "        return default or {}\n",
    "\n",
    "\n",
    "def analyze_query_node(state: AgenticRAGState) -> AgenticRAGState:\n",
    "    \"\"\"Analyze the query to determine handling strategy.\"\"\"\n",
    "    query = state[\"query\"]\n",
    "    analysis_prompt = f\"\"\"Analyze this query and respond with JSON:\n",
    "\n",
    "Query: {query}\n",
    "\n",
    "Determine:\n",
    "1. query_type: \"factual\" (specific facts), \"conceptual\" (understanding),\n",
    "   \"procedural\" (how-to), or \"comparison\" (comparing things)\n",
    "2. complexity: \"simple\" (direct answer), \"moderate\" (some context needed),\n",
    "   \"complex\" (multiple aspects, deep context)\n",
    "3. needs_retrieval: true if this needs information from documents,\n",
    "   false if general knowledge suffices\n",
    "4. search_queries: if retrieval needed, list 1-3 effective search queries\n",
    "\n",
    "Respond ONLY with JSON: {{\"query_type\": \"...\", \"complexity\": \"...\",\n",
    "\"needs_retrieval\": true/false, \"search_queries\": [...]}}\"\"\"\n",
    "\n",
    "    response = llm.invoke(analysis_prompt)\n",
    "    default = {\"query_type\": \"conceptual\", \"complexity\": \"moderate\", \n",
    "               \"needs_retrieval\": True, \"search_queries\": [query]}\n",
    "    analysis = parse_json_response(response.content, default)\n",
    "    return {\n",
    "        \"query_type\": analysis.get(\"query_type\", \"conceptual\"),\n",
    "        \"complexity\": analysis.get(\"complexity\", \"moderate\"),\n",
    "        \"needs_retrieval\": analysis.get(\"needs_retrieval\", True),\n",
    "        \"search_queries\": analysis.get(\"search_queries\", [query]),\n",
    "        \"iteration\": 0\n",
    "    }\n",
    "\n",
    "\n",
    "def retrieve_node(state: AgenticRAGState) -> AgenticRAGState:\n",
    "    \"\"\"Retrieve documents with strategy based on query complexity.\"\"\"\n",
    "    complexity = state[\"complexity\"]\n",
    "    search_queries = state[\"search_queries\"]\n",
    "    k_values = {\"simple\": 2, \"moderate\": 4, \"complex\": 6}\n",
    "    k = k_values.get(complexity, 3)\n",
    "\n",
    "    all_docs = []\n",
    "    for query in search_queries:\n",
    "        docs = vector_store.similarity_search(query, k=k)\n",
    "        all_docs.extend(docs)\n",
    "\n",
    "    seen = set()\n",
    "    unique_docs = []\n",
    "    for doc in all_docs:\n",
    "        content_hash = hash(doc.page_content)\n",
    "        if content_hash not in seen:\n",
    "            seen.add(content_hash)\n",
    "            unique_docs.append(doc)\n",
    "    return {\"retrieved_docs\": unique_docs}\n",
    "\n",
    "\n",
    "def evaluate_node(state: AgenticRAGState) -> AgenticRAGState:\n",
    "    \"\"\"Evaluate if retrieved documents can answer the query.\"\"\"\n",
    "    query = state[\"query\"]\n",
    "    docs = state[\"retrieved_docs\"]\n",
    "    iteration = state.get(\"iteration\", 0)\n",
    "    \n",
    "    # If no docs retrieved, return low confidence\n",
    "    if not docs:\n",
    "        return {\n",
    "            \"retrieval_sufficient\": iteration >= 2,\n",
    "            \"confidence\": 0.3,\n",
    "            \"iteration\": iteration + 1\n",
    "        }\n",
    "    \n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "\n",
    "    eval_prompt = f\"\"\"Evaluate if this context can answer the query:\n",
    "\n",
    "Query: {query}\n",
    "\n",
    "Retrieved Context:\n",
    "{context[:3000]}\n",
    "\n",
    "Assess:\n",
    "1. relevance: Are these documents about the right topic? (0-1)\n",
    "2. completeness: Do they contain enough to fully answer? (0-1)\n",
    "3. confidence: How confident can the answer be? (0-1)\n",
    "\n",
    "Respond with JSON: {{\"relevance\": 0.8, \"completeness\": 0.7, \"confidence\": 0.75}}\"\"\"\n",
    "\n",
    "    response = llm.invoke(eval_prompt)\n",
    "    default = {\"relevance\": 0.7, \"completeness\": 0.6, \"confidence\": 0.65}\n",
    "    scores = parse_json_response(response.content, default)\n",
    "    \n",
    "    confidence = scores.get(\"confidence\", 0.65)\n",
    "    sufficient = confidence >= 0.7 or iteration >= 2\n",
    "    return {\n",
    "        \"retrieval_sufficient\": sufficient,\n",
    "        \"confidence\": confidence,\n",
    "        \"iteration\": iteration + 1\n",
    "    }\n",
    "\n",
    "\n",
    "def generate_node(state: AgenticRAGState) -> AgenticRAGState:\n",
    "    \"\"\"Generate response using retrieved context.\"\"\"\n",
    "    query = state[\"query\"]\n",
    "    docs = state[\"retrieved_docs\"]\n",
    "    confidence = state.get(\"confidence\", 0.5)\n",
    "    context = \"\\n\\n\".join([\n",
    "        f\"[{doc.metadata.get('source', 'unknown')}]:\\n{doc.page_content}\"\n",
    "        for doc in docs\n",
    "    ])\n",
    "\n",
    "    generate_prompt = f\"\"\"Answer this query using the provided context:\n",
    "\n",
    "Query: {query}\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Instructions:\n",
    "- Answer based on the context provided\n",
    "- Cite sources when referencing specific information\n",
    "- If context is insufficient, acknowledge limitations\n",
    "- Be clear and educational in your explanation\"\"\"\n",
    "\n",
    "    response = llm.invoke(generate_prompt)\n",
    "    return {\"response\": response.content, \"confidence\": confidence}\n",
    "\n",
    "\n",
    "def direct_answer_node(state: AgenticRAGState) -> AgenticRAGState:\n",
    "    \"\"\"Answer directly without retrieval.\"\"\"\n",
    "    query = state[\"query\"]\n",
    "    response = llm.invoke(f\"Answer this question concisely: {query}\")\n",
    "    return {\"response\": response.content, \"confidence\": 0.9}\n",
    "\n",
    "\n",
    "def route_after_analysis(state: AgenticRAGState) -> Literal[\"retrieve\", \"direct\"]:\n",
    "    \"\"\"Route based on whether retrieval is needed.\"\"\"\n",
    "    if state[\"needs_retrieval\"]:\n",
    "        return \"retrieve\"\n",
    "    return \"direct\"\n",
    "\n",
    "\n",
    "def route_after_evaluation(state: AgenticRAGState) -> Literal[\"generate\", \"retrieve\"]:\n",
    "    \"\"\"Route based on whether retrieval was sufficient.\"\"\"\n",
    "    if state[\"retrieval_sufficient\"]:\n",
    "        return \"generate\"\n",
    "    return \"retrieve\"\n",
    "\n",
    "\n",
    "# Build the graph\n",
    "graph = StateGraph(AgenticRAGState)\n",
    "graph.add_node(\"analyze\", analyze_query_node)\n",
    "graph.add_node(\"retrieve\", retrieve_node)\n",
    "graph.add_node(\"evaluate\", evaluate_node)\n",
    "graph.add_node(\"generate\", generate_node)\n",
    "graph.add_node(\"direct\", direct_answer_node)\n",
    "\n",
    "graph.set_entry_point(\"analyze\")\n",
    "graph.add_conditional_edges(\"analyze\", route_after_analysis, {\n",
    "    \"retrieve\": \"retrieve\",\n",
    "    \"direct\": \"direct\"\n",
    "})\n",
    "graph.add_edge(\"retrieve\", \"evaluate\")\n",
    "graph.add_conditional_edges(\"evaluate\", route_after_evaluation, {\n",
    "    \"generate\": \"generate\",\n",
    "    \"retrieve\": \"retrieve\"\n",
    "})\n",
    "graph.add_edge(\"generate\", END)\n",
    "graph.add_edge(\"direct\", END)\n",
    "\n",
    "agentic_rag = graph.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4bf847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try it out!\n",
    "result = agentic_rag.invoke({\n",
    "    \"query\": \"What is the difference between RAG and fine-tuning?\",\n",
    "    \"messages\": []\n",
    "})\n",
    "print(f\"Query Type: {result.get('query_type')}\")\n",
    "print(f\"Complexity: {result.get('complexity')}\")\n",
    "print(f\"Used Retrieval: {result.get('needs_retrieval')}\")\n",
    "print(f\"Confidence: {result.get('confidence')}\")\n",
    "print(f\"\\nResponse:\\n{result.get('response')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tu6amfr7lwc",
   "metadata": {},
   "source": [
    "# Query Planning and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hvnj401791i",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_search_intent(query: str) -> dict:\n",
    "    \"\"\"Extract the underlying search intent from a user query.\"\"\"\n",
    "\n",
    "    intent_prompt = f\"\"\"Analyze this user query for search intent:\n",
    "\n",
    "Query: \"{query}\"\n",
    "\n",
    "Identify:\n",
    "1. core_topic: The main subject (use technical terms)\n",
    "2. specific_aspect: What specifically they want to know\n",
    "3. related_concepts: Other topics that might be relevant\n",
    "4. search_terms: 3-5 terms likely to appear in relevant documents\n",
    "\n",
    "Example:\n",
    "Query: \"How do I make my chatbot remember stuff?\"\n",
    "- core_topic: \"memory systems in LLM applications\"\n",
    "- specific_aspect: \"implementing conversation persistence\"\n",
    "- related_concepts: [\"context window\", \"vector stores\", \"session state\"]\n",
    "- search_terms: [\"memory\", \"persistence\", \"conversation history\", \"state management\"]\n",
    "\n",
    "Respond with JSON.\"\"\"\n",
    "\n",
    "    response = llm.invoke(intent_prompt)\n",
    "    return parse_json_response(response.content)\n",
    "\n",
    "\n",
    "def decompose_complex_query(query: str) -> list[str]:\n",
    "    \"\"\"Break complex queries into searchable sub-questions.\"\"\"\n",
    "\n",
    "    decompose_prompt = f\"\"\"Break this complex query into simpler sub-questions:\n",
    "\n",
    "Query: \"{query}\"\n",
    "\n",
    "Rules:\n",
    "- Each sub-question should be answerable with a focused search\n",
    "- Cover all aspects of the original query\n",
    "- Keep sub-questions independent\n",
    "- Maximum 4 sub-questions\n",
    "\n",
    "If the query is already simple, return it unchanged.\n",
    "\n",
    "Respond with JSON: {{\"sub_questions\": [\"q1\", \"q2\", ...]}}\"\"\"\n",
    "\n",
    "    response = llm.invoke(decompose_prompt)\n",
    "    result = parse_json_response(response.content)\n",
    "    return result[\"sub_questions\"]\n",
    "\n",
    "\n",
    "def assess_complexity(query: str, query_type: str) -> str:\n",
    "    \"\"\"Determine query complexity for resource allocation.\"\"\"\n",
    "\n",
    "    complexity_prompt = f\"\"\"Assess the complexity of answering this query:\n",
    "\n",
    "Query: \"{query}\"\n",
    "Query Type: {query_type}\n",
    "\n",
    "Complexity levels:\n",
    "- SIMPLE: Direct fact or definition, single concept, one source sufficient\n",
    "- MODERATE: Requires some explanation, few concepts, 2-3 sources helpful\n",
    "- COMPLEX: Multiple aspects, comparison/evaluation, needs synthesis from many sources\n",
    "\n",
    "Consider:\n",
    "- How many distinct concepts are involved?\n",
    "- Does it require comparison or evaluation?\n",
    "- Is domain expertise needed?\n",
    "- Would the answer need multiple sources?\n",
    "\n",
    "Respond with just: SIMPLE, MODERATE, or COMPLEX\"\"\"\n",
    "\n",
    "    response = llm.invoke(complexity_prompt)\n",
    "    return response.content.strip().upper()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "r90aed8sar",
   "metadata": {},
   "source": [
    "# Dynamic Retrieval Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "winzlh7vnpn",
   "metadata": {},
   "outputs": [],
   "source": [
    "def should_retrieve(query: str, query_type: str) -> bool:\n",
    "    \"\"\"Determine if retrieval would help answer this query.\"\"\"\n",
    "\n",
    "    # Quick heuristics first\n",
    "    general_patterns = [\"what is 2+2\", \"hello\", \"how are you\", \"thanks\"]\n",
    "    if any(p in query.lower() for p in general_patterns):\n",
    "        return False\n",
    "\n",
    "    # LLM-based decision for ambiguous cases\n",
    "    decision_prompt = f\"\"\"Should I search a knowledge base to answer this?\n",
    "\n",
    "Query: \"{query}\"\n",
    "\n",
    "The knowledge base contains technical documentation about AI engineering,\n",
    "including RAG, agents, LangChain, embeddings, and related topics.\n",
    "\n",
    "Answer YES if:\n",
    "- The query is about these specific topics\n",
    "- The answer requires factual information I might not have\n",
    "- The user is asking about implementation details\n",
    "\n",
    "Answer NO if:\n",
    "- It's a general knowledge question\n",
    "- It's conversational or social\n",
    "- It's about basic programming not specific to AI engineering\n",
    "- I can answer confidently from general knowledge\n",
    "\n",
    "Respond with just: YES or NO\"\"\"\n",
    "\n",
    "    response = llm.invoke(decision_prompt)\n",
    "    return \"YES\" in response.content.upper()\n",
    "\n",
    "\n",
    "def generate_search_queries(query: str, complexity: str) -> list[str]:\n",
    "    \"\"\"Generate effective search queries based on user query.\"\"\"\n",
    "\n",
    "    num_queries = {\"SIMPLE\": 1, \"MODERATE\": 2, \"COMPLEX\": 3}\n",
    "    n = num_queries.get(complexity, 2)\n",
    "\n",
    "    gen_prompt = f\"\"\"Generate {n} search queries to find information for:\n",
    "\n",
    "User question: \"{query}\"\n",
    "\n",
    "Guidelines:\n",
    "- Use technical terms that would appear in documentation\n",
    "- Make queries specific and focused\n",
    "- Cover different angles of the question\n",
    "- Each query should be 3-7 words\n",
    "\n",
    "Respond with JSON: {{\"queries\": [\"query1\", \"query2\", ...]}}\"\"\"\n",
    "\n",
    "    response = llm.invoke(gen_prompt)\n",
    "    result = parse_json_response(response.content)\n",
    "    return result[\"queries\"]\n",
    "\n",
    "\n",
    "def calculate_retrieval_k(complexity: str, query_type: str) -> int:\n",
    "    \"\"\"Calculate how many documents to retrieve.\"\"\"\n",
    "\n",
    "    base_k = {\n",
    "        \"SIMPLE\": 2,\n",
    "        \"MODERATE\": 4,\n",
    "        \"COMPLEX\": 6\n",
    "    }\n",
    "\n",
    "    # Adjust for query type\n",
    "    type_multiplier = {\n",
    "        \"comparison\": 1.5,   # Need multiple perspectives\n",
    "        \"procedural\": 1.0,   # Usually one good doc suffices\n",
    "        \"conceptual\": 1.25,  # Benefits from multiple explanations\n",
    "        \"factual\": 0.75     # Usually in one place\n",
    "    }\n",
    "\n",
    "    k = base_k.get(complexity, 3)\n",
    "    k = int(k * type_multiplier.get(query_type, 1.0))\n",
    "\n",
    "    return max(1, min(k, 10))  # Clamp between 1-10\n",
    "\n",
    "\n",
    "def multi_query_retrieve(queries: list[str], k_per_query: int) -> list:\n",
    "    \"\"\"Retrieve documents using multiple queries and deduplicate.\"\"\"\n",
    "\n",
    "    all_docs = []\n",
    "    seen_content = set()\n",
    "\n",
    "    for query in queries:\n",
    "        docs = vector_store.similarity_search(query, k=k_per_query)\n",
    "\n",
    "        for doc in docs:\n",
    "            # Deduplicate by content hash\n",
    "            content_hash = hash(doc.page_content[:500])\n",
    "            if content_hash not in seen_content:\n",
    "                seen_content.add(content_hash)\n",
    "                all_docs.append(doc)\n",
    "\n",
    "    return all_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "u3nc4ank77b",
   "metadata": {},
   "source": [
    "# Result Synthesis and Response Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2wpoapftdz3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def synthesize_response(query: str, docs: list, confidence: float) -> str:\n",
    "    \"\"\"Generate a synthesized response from multiple documents.\"\"\"\n",
    "\n",
    "    # Format documents with source attribution\n",
    "    formatted_context = []\n",
    "    for i, doc in enumerate(docs):\n",
    "        source = doc.metadata.get(\"source\", f\"Document {i+1}\")\n",
    "        formatted_context.append(f\"[Source: {source}]\\n{doc.page_content}\")\n",
    "\n",
    "    context = \"\\n\\n---\\n\\n\".join(formatted_context)\n",
    "\n",
    "    confidence_instruction = \"\"\n",
    "    if confidence < 0.7:\n",
    "        confidence_instruction = \"\"\"\n",
    "Note: Retrieved information may be incomplete. Acknowledge any gaps\n",
    "and avoid overstating certainty.\"\"\"\n",
    "\n",
    "    synthesis_prompt = f\"\"\"Answer this question by synthesizing the provided sources:\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Sources:\n",
    "{context}\n",
    "{confidence_instruction}\n",
    "\n",
    "Instructions:\n",
    "- Synthesize information from multiple sources when relevant\n",
    "- Cite sources naturally (e.g., \"According to the RAG fundamentals guide...\")\n",
    "- If sources provide different perspectives, acknowledge them\n",
    "- Be educational and clear\n",
    "- If information is incomplete, say so rather than guessing\"\"\"\n",
    "\n",
    "    response = llm.invoke(synthesis_prompt)\n",
    "    return response.content\n",
    "\n",
    "\n",
    "def check_for_conflicts(docs: list) -> list[str]:\n",
    "    \"\"\"Identify potential conflicts in retrieved documents.\"\"\"\n",
    "\n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "\n",
    "    conflict_prompt = f\"\"\"Review these documents for conflicting information:\n",
    "\n",
    "{context}\n",
    "\n",
    "Identify any contradictory claims or recommendations.\n",
    "If conflicts exist, explain what differs and why both might be valid.\n",
    "\n",
    "Respond with JSON: {{\"has_conflicts\": true/false, \"conflicts\": [...]}}\"\"\"\n",
    "\n",
    "    response = llm.invoke(conflict_prompt)\n",
    "    result = parse_json_response(response.content)\n",
    "    return result.get(\"conflicts\", [])\n",
    "\n",
    "\n",
    "def calculate_confidence(query: str, docs: list, relevance_score: float) -> float:\n",
    "    \"\"\"Calculate confidence score for the response.\"\"\"\n",
    "\n",
    "    # Factor 1: Retrieval relevance\n",
    "    retrieval_confidence = relevance_score\n",
    "\n",
    "    # Factor 2: Coverage - do docs address the query's main aspects?\n",
    "    coverage_prompt = f\"\"\"Rate how well these documents cover this query (0-1):\n",
    "Query: {query}\n",
    "Docs cover: {[doc.page_content[:200] for doc in docs]}\n",
    "Respond with just a number.\"\"\"\n",
    "\n",
    "    coverage = float(llm.invoke(coverage_prompt).content.strip())\n",
    "\n",
    "    # Factor 3: Consistency - do docs agree?\n",
    "    if len(docs) > 1:\n",
    "        consistency = 0.9  # Assume consistent unless conflicts detected\n",
    "    else:\n",
    "        consistency = 0.8  # Single source is less reliable\n",
    "\n",
    "    # Weighted combination\n",
    "    confidence = (\n",
    "        0.4 * retrieval_confidence +\n",
    "        0.4 * coverage +\n",
    "        0.2 * consistency\n",
    "    )\n",
    "\n",
    "    return round(confidence, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md1e52o84jb",
   "metadata": {},
   "source": [
    "# Handling Ambiguity and Uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "t18vler0qca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_ambiguity(query: str, conversation_history: list) -> dict:\n",
    "    \"\"\"Detect if query is ambiguous and might need clarification.\"\"\"\n",
    "\n",
    "    ambiguity_prompt = f\"\"\"Analyze this query for ambiguity:\n",
    "\n",
    "Query: \"{query}\"\n",
    "Conversation context: {conversation_history[-3:] if conversation_history else 'None'}\n",
    "\n",
    "Check for:\n",
    "- Unclear pronouns (\"it\", \"this\", \"that\") without referent\n",
    "- Missing context that makes the query interpretable multiple ways\n",
    "- Vague terms that could mean different things\n",
    "- References to prior conversation that isn't available\n",
    "\n",
    "Respond with JSON:\n",
    "{{\n",
    "    \"is_ambiguous\": true/false,\n",
    "    \"ambiguity_type\": \"pronoun/context/vague/reference\" or null,\n",
    "    \"clarifying_question\": \"What would help?\" or null,\n",
    "    \"best_interpretation\": \"Most likely meaning if we proceed\"\n",
    "}}\"\"\"\n",
    "\n",
    "    response = llm.invoke(ambiguity_prompt)\n",
    "    return parse_json_response(response.content)\n",
    "\n",
    "\n",
    "def should_clarify(ambiguity_analysis: dict) -> bool:\n",
    "    \"\"\"Decide whether to ask for clarification.\"\"\"\n",
    "\n",
    "    if not ambiguity_analysis[\"is_ambiguous\"]:\n",
    "        return False\n",
    "\n",
    "    # High-risk ambiguities warrant clarification\n",
    "    high_risk_types = [\"pronoun\", \"reference\"]\n",
    "    if ambiguity_analysis[\"ambiguity_type\"] in high_risk_types:\n",
    "        return True\n",
    "\n",
    "    # For vague queries, proceed with best interpretation\n",
    "    # but note the assumption in the response\n",
    "    return False\n",
    "\n",
    "\n",
    "def generate_uncertain_response(query: str, confidence: float, reason: str) -> str:\n",
    "    \"\"\"Generate response that acknowledges uncertainty.\"\"\"\n",
    "\n",
    "    uncertain_prompt = f\"\"\"Generate a helpful response that acknowledges limitations:\n",
    "\n",
    "Query: {query}\n",
    "Confidence level: {confidence}\n",
    "Uncertainty reason: {reason}\n",
    "\n",
    "Guidelines:\n",
    "- Be honest about what you don't know\n",
    "- Share whatever relevant information you do have\n",
    "- Suggest how the user might find better information\n",
    "- Don't apologize excessively, just be straightforward\n",
    "\n",
    "Example: \"I found some information about X, but the documents don't directly\n",
    "address Y. Here's what I can tell you... For more specific information about Y,\n",
    "you might want to...\"\"\"\n",
    "\n",
    "    response = llm.invoke(uncertain_prompt)\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4hr9omm73wd",
   "metadata": {},
   "source": [
    "# Putting It All Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9i978qwk6gl",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Complete Agentic RAG Pipeline\n",
    "# First, load and index the documents from the documents directory\n",
    "\n",
    "from langchain_community.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Load all markdown files from the documents directory\n",
    "loader = DirectoryLoader(\n",
    "    \"documents\",\n",
    "    glob=\"**/*.md\",\n",
    "    loader_cls=TextLoader\n",
    ")\n",
    "documents = loader.load()\n",
    "print(f\"Loaded {len(documents)} documents\")\n",
    "\n",
    "# Split into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200\n",
    ")\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "print(f\"Split into {len(chunks)} chunks\")\n",
    "\n",
    "# Add to vector store\n",
    "vector_store.add_documents(chunks)\n",
    "print(f\"Indexed {len(chunks)} chunks in vector store\\n\")\n",
    "\n",
    "# Now run the agentic RAG pipeline\n",
    "test_query = \"How does RAG compare to fine-tuning for customizing LLM behavior?\"\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Running Agentic RAG Pipeline\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "result = agentic_rag.invoke({\n",
    "    \"query\": test_query,\n",
    "    \"messages\": []\n",
    "})\n",
    "\n",
    "print(f\"Query: {test_query}\\n\")\n",
    "print(f\"Query type: {result.get('query_type')}\")\n",
    "print(f\"Complexity: {result.get('complexity')}\")\n",
    "print(f\"Used retrieval: {result.get('needs_retrieval')}\")\n",
    "print(f\"Documents retrieved: {len(result.get('retrieved_docs', []))}\")\n",
    "print(f\"Confidence: {result.get('confidence')}\")\n",
    "print(f\"Iterations: {result.get('iteration')}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"RESPONSE\")\n",
    "print(\"=\" * 60)\n",
    "print(result.get('response'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4321ac",
   "metadata": {},
   "source": [
    "# Using Open-Source Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c63f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify Ollama is running and models are available\n",
    "from langchain_ollama import ChatOllama, OllamaEmbeddings\n",
    "\n",
    "# Test connection to Ollama\n",
    "try:\n",
    "    # Try a smaller model first (3B instead of 20B)\n",
    "    test_llm = ChatOllama(model=\"llama3.2:3b\", temperature=0)\n",
    "    test_response = test_llm.invoke(\"Say 'Ollama is working!' in exactly 3 words.\")\n",
    "    print(f\"Chat Model Test: {test_response.content}\")\n",
    "    \n",
    "    test_embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "    test_vector = test_embeddings.embed_query(\"test\")\n",
    "    print(f\"Embedding Model Test: Vector dimension = {len(test_vector)}\")\n",
    "    print(\"\\nOllama is ready!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error connecting to Ollama: {e}\")\n",
    "    print(\"\\nMake sure:\")\n",
    "    print(\"1. Ollama is installed: https://ollama.com/\")\n",
    "    print(\"2. Ollama is running: 'ollama serve'\")\n",
    "    print(\"3. Models are pulled: 'ollama pull llama3.2:3b' and 'ollama pull nomic-embed-text'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "v4-agentic-rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
