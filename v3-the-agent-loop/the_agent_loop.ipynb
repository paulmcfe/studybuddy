{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# The Agent Loop\n",
    "\n",
    "This notebook demonstrates how to build an AI agent using LangChain 1.0's `create_agent` API with Qdrant for vector storage. We'll:\n",
    "\n",
    "1. **Set up Qdrant** - Create an in-memory vector database\n",
    "2. **Build a document processing pipeline** - Load, chunk, and index documents\n",
    "3. **Create tools** - Define a search tool the agent can use\n",
    "4. **Build the agent** - Use LangChain's create_agent with a system prompt\n",
    "5. **Extract reasoning** - Parse the agent's message history to see its \"thinking\"\n",
    "\n",
    "## Setup\n",
    "\n",
    "Before running this notebook, make sure you have:\n",
    "- An OpenAI API key set as the `OPENAI_API_KEY` environment variable\n",
    "- The required packages installed: `uv pip install langchain langchain-openai langchain-qdrant langchain-community langchain-text-splitters qdrant-client python-dotenv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Verify API key is set\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    raise ValueError(\"Please set OPENAI_API_KEY environment variable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## Setting up Qdrant\n",
    "\n",
    "Qdrant is a vector database that stores embeddings and enables similarity search. We're using in-memory mode for simplicity, but Qdrant can also run as a persistent server or cloud service.\n",
    "\n",
    "Key concepts:\n",
    "- **Collection**: A named set of vectors (like a table in a traditional database)\n",
    "- **Vector dimensions**: Must match your embedding model (1536 for text-embedding-3-small)\n",
    "- **Distance metric**: Cosine similarity is standard for text embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_qdrant import QdrantVectorStore\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import Distance, VectorParams\n",
    "\n",
    "# Initialize embeddings\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "# Create in-memory Qdrant client\n",
    "qdrant_client = QdrantClient(\":memory:\")\n",
    "\n",
    "# Collection name\n",
    "COLLECTION_NAME = \"study_materials\"\n",
    "\n",
    "# Create collection with proper dimensions (1536 for text-embedding-3-small)\n",
    "qdrant_client.create_collection(\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    vectors_config=VectorParams(size=1536, distance=Distance.COSINE)\n",
    ")\n",
    "\n",
    "# Create LangChain vector store wrapper\n",
    "vector_store = QdrantVectorStore(\n",
    "    client=qdrant_client,\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    embedding=embeddings\n",
    ")\n",
    "\n",
    "print(f\"Created Qdrant collection: {COLLECTION_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## Document Processing Pipeline\n",
    "\n",
    "Before we can search documents, we need to:\n",
    "1. **Load** - Read the document from disk\n",
    "2. **Chunk** - Split into smaller pieces that fit in context windows\n",
    "3. **Embed** - Convert text to vectors\n",
    "4. **Index** - Store in the vector database\n",
    "\n",
    "LangChain provides loaders and splitters that handle this pipeline. The `RecursiveCharacterTextSplitter` tries to split on natural boundaries (paragraphs, sentences) rather than cutting mid-word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "def index_document(file_path: str, doc_name: str):\n",
    "    \"\"\"Load, chunk, and index a document.\"\"\"\n",
    "    loader = TextLoader(file_path)\n",
    "    documents = loader.load()\n",
    "\n",
    "    # Split into chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=500,\n",
    "        chunk_overlap=50,\n",
    "        length_function=len\n",
    "    )\n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "    # Add metadata\n",
    "    for chunk in chunks:\n",
    "        chunk.metadata['source'] = doc_name\n",
    "\n",
    "    # Index in Qdrant\n",
    "    vector_store.add_documents(chunks)\n",
    "\n",
    "    return len(chunks)\n",
    "\n",
    "print(\"Document indexing function ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## Index Sample Documents\n",
    "\n",
    "Let's index all `.txt` and `.md` files from the documents directory. In a real application, this might run in the background on server startup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find documents directory\n",
    "documents_dir = Path(\"documents\")\n",
    "if not documents_dir.exists():\n",
    "    documents_dir = Path(\"../documents\")  # Try parent if running from api/\n",
    "\n",
    "if documents_dir.exists():\n",
    "    # Get all .txt and .md files\n",
    "    story_files = sorted(\n",
    "        list(documents_dir.glob(\"*.txt\")) +\n",
    "        list(documents_dir.glob(\"*.md\"))\n",
    "    )\n",
    "    \n",
    "    print(f\"Found {len(story_files)} documents to index\\n\")\n",
    "    \n",
    "    total_chunks = 0\n",
    "    for filepath in story_files:\n",
    "        doc_name = filepath.stem.replace(\"-\", \" \").replace(\"_\", \" \").title()\n",
    "        num_chunks = index_document(str(filepath), doc_name)\n",
    "        total_chunks += num_chunks\n",
    "        print(f\"Indexed {num_chunks} chunks from {doc_name}\")\n",
    "    \n",
    "    print(f\"\\nTotal: {total_chunks} chunks indexed\")\n",
    "else:\n",
    "    print(\"Documents directory not found. Create a 'documents' folder with .txt or .md files.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## Creating Tools\n",
    "\n",
    "An agent needs tools to interact with the world. The `@tool` decorator turns a Python function into something the agent can call. The docstring is crucial - it gets passed to the LLM so it knows when and how to use the tool.\n",
    "\n",
    "Our search tool:\n",
    "- Takes a query string\n",
    "- Searches the vector store for similar chunks\n",
    "- Returns formatted results with source attribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "\n",
    "@tool\n",
    "def search_materials(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Search the indexed study materials for information about a topic.\n",
    "    Use this when you need to find specific information from the\n",
    "    student's uploaded study materials.\n",
    "\n",
    "    Args:\n",
    "        query: The search term or question to look up\n",
    "    \"\"\"\n",
    "    results = vector_store.similarity_search(query, k=3)\n",
    "\n",
    "    if not results:\n",
    "        return \"No relevant information found in study materials.\"\n",
    "\n",
    "    # Format results with source attribution\n",
    "    formatted = []\n",
    "    for doc in results:\n",
    "        source = doc.metadata.get('source', 'Unknown')\n",
    "        formatted.append(f\"[From {source}]:\\n{doc.page_content}\")\n",
    "\n",
    "    return \"\\n\\n\".join(formatted)\n",
    "\n",
    "# Test the tool directly\n",
    "print(\"Testing search tool...\")\n",
    "print(search_materials.invoke(\"Irene Adler\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## Building the Agent\n",
    "\n",
    "Now we create the agent using LangChain 1.0's `create_agent` API. The key components:\n",
    "\n",
    "1. **Model**: The LLM that powers reasoning (gpt-4o-mini)\n",
    "2. **Tools**: Functions the agent can call\n",
    "3. **System prompt**: Instructions that shape the agent's behavior\n",
    "\n",
    "The system prompt is critical. It tells the agent:\n",
    "- What role it plays (StudyBuddy tutoring assistant)\n",
    "- When to use tools (questions about study materials)\n",
    "- When NOT to use tools (general knowledge questions)\n",
    "- How to format responses (cite sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "\n",
    "# System prompt\n",
    "SYSTEM_PROMPT = \"\"\"You are StudyBuddy, an AI tutoring assistant helping students study Sherlock Holmes stories.\n",
    "\n",
    "IMPORTANT: The student has uploaded study materials (Sherlock Holmes stories) that you MUST search\n",
    "before answering questions about characters, plots, or events. Always use the search_materials tool\n",
    "first for any question about:\n",
    "- Characters (Holmes, Watson, Irene Adler, Moriarty, etc.)\n",
    "- Story plots or mysteries\n",
    "- Specific events or quotes\n",
    "- Anything related to the study materials\n",
    "\n",
    "Only answer from general knowledge for questions completely unrelated to the study materials\n",
    "(like math questions or general facts).\n",
    "\n",
    "When you search, cite which story the information comes from.\"\"\"\n",
    "\n",
    "# Create tools list\n",
    "tools = [search_materials]\n",
    "\n",
    "# Create the agent using LangChain 1.0 API\n",
    "agent = create_agent(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    tools=tools,\n",
    "    system_prompt=SYSTEM_PROMPT\n",
    ")\n",
    "\n",
    "print(\"Agent created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## Invoking the Agent\n",
    "\n",
    "When we invoke the agent, it returns a response containing the full message history. This history shows us exactly what the agent did:\n",
    "\n",
    "1. **HumanMessage**: The user's question\n",
    "2. **AIMessage with tool_calls**: The agent deciding to use a tool\n",
    "3. **ToolMessage**: The result from executing the tool\n",
    "4. **AIMessage with content**: The final answer\n",
    "\n",
    "LangChain 1.0 uses native tool calling (structured function calls) rather than text-based ReAct reasoning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask a question that should trigger search\n",
    "response = agent.invoke({\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"Who is Irene Adler?\"}]\n",
    "})\n",
    "\n",
    "# Look at the raw response structure\n",
    "print(\"Message history:\")\n",
    "print(\"-\" * 50)\n",
    "for msg in response[\"messages\"]:\n",
    "    msg_type = getattr(msg, 'type', 'unknown')\n",
    "    print(f\"\\n[{msg_type.upper()}]\")\n",
    "    \n",
    "    if hasattr(msg, 'tool_calls') and msg.tool_calls:\n",
    "        for tc in msg.tool_calls:\n",
    "            print(f\"  Tool call: {tc['name']}({tc['args']})\")\n",
    "    \n",
    "    if msg.content:\n",
    "        content_preview = msg.content[:200] + \"...\" if len(msg.content) > 200 else msg.content\n",
    "        print(f\"  Content: {content_preview}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## Extracting Reasoning\n",
    "\n",
    "For transparency, we want to show users what the agent did - which tools it called, what queries it made, and what results it found. This builds trust and helps debug issues.\n",
    "\n",
    "We parse the message history to extract:\n",
    "- **Actions**: Which tools were called and with what inputs\n",
    "- **Observations**: What the tools returned\n",
    "- **Final Answer**: The agent's response to the user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_reasoning(response):\n",
    "    \"\"\"Extract reasoning trace and final answer from agent response.\"\"\"\n",
    "    messages = response[\"messages\"]\n",
    "    reasoning_parts = []\n",
    "    final_answer = \"\"\n",
    "\n",
    "    for msg in messages:\n",
    "        msg_type = getattr(msg, 'type', None)\n",
    "\n",
    "        # Skip the user message\n",
    "        if msg_type == 'human':\n",
    "            continue\n",
    "\n",
    "        # Check for tool calls (agent deciding to use a tool)\n",
    "        if msg_type == 'ai' and hasattr(msg, 'tool_calls') and msg.tool_calls:\n",
    "            for tool_call in msg.tool_calls:\n",
    "                reasoning_parts.append(f\"Action: {tool_call['name']}\")\n",
    "                reasoning_parts.append(f\"Input: {tool_call['args']}\")\n",
    "\n",
    "        # Check for tool responses\n",
    "        elif msg_type == 'tool':\n",
    "            tool_name = getattr(msg, 'name', 'search_materials')\n",
    "            content = msg.content if msg.content else ''\n",
    "            # Truncate long content\n",
    "            display_content = content[:500] + '...' if len(content) > 500 else content\n",
    "            reasoning_parts.append(f\"Observation from {tool_name}:\\n{display_content}\")\n",
    "\n",
    "        # The final AI message is the answer (has content but no tool calls)\n",
    "        elif msg_type == 'ai' and msg.content:\n",
    "            tool_calls = getattr(msg, 'tool_calls', [])\n",
    "            if not tool_calls:\n",
    "                final_answer = msg.content\n",
    "\n",
    "    reasoning = \"\\n\\n\".join(reasoning_parts) if reasoning_parts else None\n",
    "    return {\"answer\": final_answer, \"reasoning\": reasoning}\n",
    "\n",
    "# Test with a question that requires search\n",
    "response = agent.invoke({\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"What happened in A Scandal in Bohemia?\"}]\n",
    "})\n",
    "\n",
    "result = extract_reasoning(response)\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"REASONING TRACE\")\n",
    "print(\"=\" * 50)\n",
    "print(result[\"reasoning\"])\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"FINAL ANSWER\")\n",
    "print(\"=\" * 50)\n",
    "print(result[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## Testing Different Question Types\n",
    "\n",
    "Let's see how the agent handles different types of questions:\n",
    "\n",
    "1. **Questions about study materials** - Should trigger search\n",
    "2. **General knowledge questions** - Should answer directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_agent(question: str):\n",
    "    \"\"\"Helper function to ask the agent and display results.\"\"\"\n",
    "    print(f\"\\nQuestion: {question}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    response = agent.invoke({\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": question}]\n",
    "    })\n",
    "    \n",
    "    result = extract_reasoning(response)\n",
    "    \n",
    "    if result[\"reasoning\"]:\n",
    "        print(\"[Used tools]\")\n",
    "    else:\n",
    "        print(\"[Answered directly - no tools used]\")\n",
    "    \n",
    "    print(f\"\\nAnswer: {result['answer'][:500]}...\" if len(result['answer']) > 500 else f\"\\nAnswer: {result['answer']}\")\n",
    "    return result\n",
    "\n",
    "# Questions that should trigger search\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"QUESTIONS ABOUT STUDY MATERIALS (should use search)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "ask_agent(\"Who is Irene Adler and why is she significant?\")\n",
    "ask_agent(\"What was the mystery in The Red-Headed League?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Questions that should be answered directly\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"GENERAL KNOWLEDGE QUESTIONS (should NOT use search)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "ask_agent(\"What is 2 + 2?\")\n",
    "ask_agent(\"What is the capital of France?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "## What's Next?\n",
    "\n",
    "This notebook demonstrated the core concepts of building an AI agent:\n",
    "\n",
    "1. **Vector storage** with Qdrant for semantic search\n",
    "2. **Document processing** with LangChain loaders and splitters\n",
    "3. **Tool creation** with the `@tool` decorator\n",
    "4. **Agent creation** with `create_agent`\n",
    "5. **Reasoning extraction** from the message history\n",
    "\n",
    "The full StudyBuddy v3 application wraps this in a FastAPI server with:\n",
    "- Background document indexing\n",
    "- Status polling endpoint\n",
    "- Chat interface with \"Show reasoning\" toggle\n",
    "\n",
    "In the next chapter, we'll rebuild using LangGraph for complete control over the agent's state machine, adding reflection, confidence scoring, and observability."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
